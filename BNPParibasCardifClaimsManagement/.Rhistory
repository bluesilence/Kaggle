library(ggplot2) # Data visualization
library(readr) # CSV file I/O, e.g. the read_csv function
install.packages('readr')
install.packages('xgboost')
system("ls ../input")
getwd()
setwd('D:\Kaggle\BNPParibasCardifClaimsManagement')
setwd('D:/Kaggle/BNPParibasCardifClaimsManagement')
train <- read.table("data/raw/train.csv", header=T, sep=",")
head(train)
y <- train[, 'target']
train <- train[, -2]
test <- read.table("data/raw/test.csv", header=T, sep=",")
train[is.na(train)] <- -1
test[is.na(test)] <- -1
f <- c()
for(i in 1:ncol(train)) {
if (is.factor(train[, i])) f <- c(f, i)
}
f.t <- c()
for(i in 1:ncol(test)) {
if (is.factor(test[, i])) f.t <- c(f.t, i)
}
ttrain <- rbind(train, test)
for (col in f) {
ttrain[, col] <- as.numeric(ttrain[, col])
}
train <- ttrain[1:nrow(train), ]
test <- ttrain[(nrow(train)+1):nrow(ttrain), ]
doTest <- function(y, train, test, param0, iter) {
n<- nrow(train)
xgtrain <- xgb.DMatrix(as.matrix(train), label = y)
xgval = xgb.DMatrix(as.matrix(test))
watchlist <- list('train' = xgtrain)
model = xgb.train(
nrounds = iter
, params = param0
, data = xgtrain
, watchlist = watchlist
, print.every.n = 100
, nthread = 8
)
p <- predict(model, xgval)
rm(model)
gc()
p
}
param0 <- list(
# general , non specific params - just guessing
"objective"  = "binary:logistic"
, "eval_metric" = "logloss"
, "eta" = 0.01
, "subsample" = 0.8
, "colsample_bytree" = 0.8
, "min_child_weight" = 1
, "max_depth" = 10
)
submission <- read.table("data/raw/sample_submission.csv", header=TRUE, sep=',')
ensemble <- rep(0, nrow(test))
ensemble
for (i in 1:5) {
p <- doTest(y, train, test, param0, 100)
# change to 1300 or 1200, test by trial and error, have to add to local check which suggests 900,
# but have another 20% training data to consider which gives longer optimal training time
ensemble <- ensemble + p
}
library(xgboost)
for (i in 1:5) {
p <- doTest(y, train, test, param0, 100)
# change to 1300 or 1200, test by trial and error, have to add to local check which suggests 900,
# but have another 20% training data to consider which gives longer optimal training time
ensemble <- ensemble + p
}
submission$PredictedProb <- ensemble / i
write.csv(submission, "data/final/xgboost_forum_submission.csv", row.names=F, quote=F)
install.packages('devtools')
devtools::install_github("JohnLangford/vowpal_wabbit", subdir = "R/r.vw")
library(r.vw)
vw-varinfo
install.packages('lattice')
install.packages('lattice')
library(lattice)
setwd('D:/Kaggle/BNPParibasCardifClaimsManagement')
train <- read.table("data/raw/train.csv", header=T, sep=",")
head(train)
train[is.na(train)] <- -1
head(train)
?pch
?pairs
pairs(train[2:length(train)])
pairs(train[2:length(train)], pch = 21)
head(train[2:length(train)])
head(train[3:length(train)])
pairs(train[3:length(train)])
pairs(iris[1:4], pch = 21)
pairs(train[3:10])
pairs(train[3:10])
